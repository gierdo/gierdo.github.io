---
title: Local AI assistant in vim
---

## The Problem

I have been using [Bryley's neoai.vim](https://github.com/Bryley/neoai.nvim) in
combination with OpenAI's gpt API for a while now, but mainly for playing
around and creating regexes, to be honest..

I do most of my coding for my employer, and they are understandably sceptical
when it comes to pushing all of their sensitive code and content to a black-box
cloud service. Which means that gpt, copilot et al. are forbidden for sensitive
usecases. Which I think is understandable. Additionally, even for personal
projects, I don't like the idea of not being in control over what is sent
where, when and what's going to happen with what is being sent.

## Possible solution

There are large language models that can be executed/operated locally!
However, it's not as easy as visiting a website or "buying" an API key.
And glueing it all together always looked a bit challenging.

I wanted to play around with
[llama](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) for a
long while now, and today finally was the day!

## Solution

I created an integrated configuration/solution which

- Uses nvim + [a fork of neoai.vim](https://github.com/gierdo/neoai.nvim)
- Starts a containerized instance of [llama.cpp](https://github.com/ggerganov/llama.cpp) with an [OpenAI compatible API](https://github.com/abetlen/llama-cpp-python)
- Uses of the [codellama language model](https://github.com/facebookresearch/codellama)
- Sets everything up automatically

### Automation

I have been using a [dotfiles repository](https://github.com/gierdo/dotfiles)
for a long time now, making use of
[tuning](https://gitlab.com/jokeyrhyme/tuning/) for the setup automation.
Everything is in the code and should bootstrap automagically.

### Downloading the model

The model is downloaded by running `tuning`, but using a little
[script](https://github.com/gierdo/dotfiles/blob/master/gists/install-llama-models.sh).

### Running the API Service

The llama API service runs in a `podman` container, orchestrated and managed by `systemd`.
The quadlet configuration is part of the [repo](https://github.com/gierdo/dotfiles/blob/master/.config/containers/systemd/llama.container)

With the configuration and model in place, the API can be brought up with a
simple

```bash
$ systemctl --user start llama
```

After that, it listens at `http://localhost:8081`

However, the nvim integration starts it automatically, if needed.

### nvim integration

There is a pretty great nvim plugin for the OpenAI API:
[neoai.nvim](https://github.com/Bryley/neoai.nvim).
I created a fork, which allows it to use an unauthenticated API on localhost:8081.
It's not nice and just adds another hard-coded bit of lua to another hardcoded
bit of lua, and both should probably be consolidated with a bit of abstraction.
However, I wanted to start somewhere, and I've never done anything in lua.
Also, I don't know if Bryley is interested in something like that, so, well.
[Here](https://github.com/gierdo/neoai.nvim/blob/local-llama/lua/neoai/chat/models/llamacpp.lua)
it is.

I install it in
[init.vim](https://github.com/gierdo/dotfiles/blob/master/.config/nvim/init.vim)
with

```text
Plug 'gierdo/neoai.nvim', { 'branch': 'local-llama' }
```

Configuration happens in the [neoai.vim configuration
file](https://github.com/gierdo/dotfiles/blob/master/vimrc.d/neoai.vim), with
automated systemd start of the API server llama.

```text
.
.
.
    models = {
        {
            name = "llamacpp",
            model = "",
            params = nil,
        },
.
.
.

function StartLlama()
    !systemctl --user start llama
endfunction

nnoremap <A-a> :execute StartLlama() \| NeoAI <CR>
vnoremap <A-a> :NeoAIContext<CR>
vnoremap <A-i> :execute StartLlama() \| NeoAIInjectContext<Space>
nnoremap <A-i> :execute StartLlama() \| NeoAIInject<Space>
inoremap <A-a> <Esc>:execute StartLlama() \| NeoAIInject<Space>
```
