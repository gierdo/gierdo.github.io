---
title: Local AI assistant in vim
categories:
  - programming
  - linux
tags:
  - vim
  - openai
  - ai
  - containers
  - llama
---

# Local AI assistant in vim with codellama

<script async id="asciicast-623203" src="https://asciinema.org/a/623203.js"></script>

## The Problem

I have been using [Bryley's neoai.vim](https://github.com/Bryley/neoai.nvim) in
combination with OpenAI's gpt API for a while now, but mainly for playing
around and creating regexes, to be honest..

I do most of my coding for my employer, and they are understandably sceptical
when it comes to pushing all of their sensitive code and content to a black-box
cloud service. Which means that gpt, copilot et al. are forbidden for sensitive
usecases. Which I think is understandable. Additionally, even for personal
projects, I don't like the idea of not being in control over what is sent
where, when and what's going to happen with what is being sent.

## Possible solution

There are large language models that can be executed/operated locally!
However, it's not as easy as visiting a website or "buying" an API key.
And glueing it all together always looked a bit challenging.

I wanted to play around with
[llama](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) for a
long while now, and today finally was the day!

## Solution

I created an integrated configuration/solution which

- Uses nvim + [a fork of neoai.vim](https://github.com/gierdo/neoai.nvim)
- Starts a containerized instance of [llama.cpp](https://github.com/ggerganov/llama.cpp) with an [OpenAI compatible API](https://github.com/abetlen/llama-cpp-python)
- Uses of the [codellama language model](https://github.com/facebookresearch/codellama)
- Sets everything up automatically

### Automation

I have been using a [dotfiles repository](https://github.com/gierdo/dotfiles)
for a long time now, making use of
[tuning](https://gitlab.com/jokeyrhyme/tuning/) for the setup automation.
Everything is in the code and should bootstrap automagically.

### Downloading the model

The model is downloaded by running `tuning`, but using a little
[script](https://github.com/gierdo/dotfiles/blob/master/gists/install-llama-models.sh).

### Running the API Service

The llama API service runs in a `podman` container, orchestrated and managed by `systemd`.
The quadlet configuration is part of the [repo](https://github.com/gierdo/dotfiles/blob/master/.config/containers/systemd/llama.container)

With the configuration and model in place, the API can be brought up with a
simple

```bash
$ systemctl --user start llama
```

After that, it listens at `http://localhost:9741`

However, the nvim integration starts it automatically, if needed.

### nvim integration

There is a pretty great nvim plugin for the OpenAI API:
[neoai.nvim](https://github.com/Bryley/neoai.nvim). I created a fork, which
allows it to use an unauthenticated API on a configurable url, e.g.
`http://localhost:9741/v1/chat/completions`. The first attempt was not nice and
just added a hard-coded bit of lua to another hardcoded bit of lua, but now
it's configurable, the merge request is open.

I install it in
[init.vim](https://github.com/gierdo/dotfiles/blob/master/.config/nvim/init.vim)
with

```text
Plug 'gierdo/neoai.nvim', { 'branch': 'local-llama' }
```

Configuration happens in the [neoai.vim configuration
file](https://github.com/gierdo/dotfiles/blob/master/vimrc.d/neoai.vim), with
automated systemd start of the API server llama.

```text
.
.
.
    models = {
        {
            name = "openai",
            model = "codellama",
            params = nil,
        },
        .
        .
        .
    open_ai = {
        url = "http://localhost:9741/v1/chat/completions",
        display_name = "llama.cpp",
.
.
.

function StartLlama()
    !systemctl --user start llama
endfunction

nnoremap <A-a> :execute StartLlama() \| NeoAI <CR>
vnoremap <A-a> :NeoAIContext<CR>
vnoremap <A-i> :execute StartLlama() \| NeoAIInjectContext<Space>
nnoremap <A-i> :execute StartLlama() \| NeoAIInject<Space>
inoremap <A-a> <Esc>:execute StartLlama() \| NeoAIInject<Space>
```

## Limitations

The proposed solution only uses CPU vectorization and should (TM) be portable.
I don't have a machine with a GPU capable of doing shiny Cuda/hipBLAS/ROCm
acceleration. This means that the performance is OK(ish) for one single
parallel user. Which is exactly what the setup was intended for from the start.
Be aware though that the running model needs 9 GiB of memory while idling and
will make use of pretty much all of your CPU while working. That's also why I
set it up to only start when it's really used.

If you're done using it and you want to free up the resources again, shut it
down with

```bash
$ systemctl --user stop llama
```

If your setup has a ROCm supported AMD graphics card or a CUDA supported Nvidia
card, you will probably want to spend some time in getting `llama.cpp` to make
use of it for a significant performance increase, at the price of removed
portability and added maintenance.
